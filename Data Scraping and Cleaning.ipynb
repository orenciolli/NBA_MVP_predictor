{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885b010d",
   "metadata": {},
   "source": [
    "# Data Scraping for MVP Predictor\n",
    "For this project, we'll be scraping data from basketball-reference.com.  \n",
    "  \n",
    "There are a few different pages we'll be interested in for every given year:\n",
    "- The page containing all players who received votes for the MVP award\n",
    "- The page with every player's basic stats (points, assists, rebounds etc.)\n",
    "- The page with every player's advanced stats (player efficiency rating, value over replacment, etc.)\n",
    "- The page containing the NBA's standings and team statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97b3b0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd8be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa10f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1980, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797daa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ec321",
   "metadata": {},
   "source": [
    "## Scraping MVP votes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb209ae",
   "metadata": {},
   "source": [
    "First, we'll save the data locally to minimize the number of requests we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = f\"https://www.basketball-reference.com/awards/awards_{year}.html\"\n",
    "    mvp_votes = requests.get(url, headers = headers).text\n",
    "    \n",
    "    with open(f'Data/mvp_votes_{year}', \"w+\") as f:\n",
    "        f.write(mvp_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9986f",
   "metadata": {},
   "source": [
    "Now, we'll read in the data for every year as a Pandas dataframe and concatenate all these dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_dfs = []\n",
    "for year in years:\n",
    "    with open(f'Data/mvp_votes_{year}') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    soup.find('tr', class_ = 'over_header').decompose()\n",
    "    \n",
    "    tab_html = soup.find(id = 'mvp')\n",
    "    table = pd.read_html(str(tab_html))[0]\n",
    "    \n",
    "    table['Year'] = year\n",
    "    \n",
    "    mvp_dfs.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mvp = pd.concat(mvp_dfs, axis = 0)\n",
    "full_mvp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65adf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving as CSV\n",
    "full_mvp.to_csv('data/full_mvp.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76662e6a",
   "metadata": {},
   "source": [
    "## Scraping Basic Stats \n",
    "Now we can scrape the basic stats. We'll use selenium along with a webdriver to do this.  \n",
    "Repeating the same process, we'll save the data locally, then read it back in as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4922da",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path = '/Users/orenciolli/Downloads/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0672a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{year}_per_game.html'\n",
    "    driver.get(url)\n",
    "    driver.execute_script('window.scrollTo(1, 10000)')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    with open(f'player_stats_{year}.html', 'w+') as f:\n",
    "        f.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf69302",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = []\n",
    "for year in years: \n",
    "    with open(f'player_stats_{year}.html') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    soup.find('tr', class_ = 'thead').decompose()\n",
    "\n",
    "    tab_html = soup.find(id = 'per_game_stats')\n",
    "    table = pd.read_html(str(tab_html))[0]\n",
    "\n",
    "    table['Year'] = year\n",
    "\n",
    "    basic_stats.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_basic_stats = pd.concat(basic_stats, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_basic_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f55ad",
   "metadata": {},
   "source": [
    "It's worth noting that if a player was traded mid-season, they will have multiple rows in that year: one for each team they played with and one representing their total stats. We'll only keep the total row.  \n",
    "Additionally, since we will later need to merge on the `team` column, we'll replace the string 'TOT' (representing total) with the last team that player played for in the season in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_traded(df):\n",
    "    if df.shape[0] > 1:\n",
    "        total = df[df['Tm'] == 'TOT']\n",
    "        total['Tm'] = df.iloc[-1,:]['Tm']\n",
    "        return total\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying to the dataframe\n",
    "full_basic_stats = full_basic_stats.groupby(['Player', 'Year']).apply(player_traded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_basic_stats.index = full_basic_stats.index.droplevel()\n",
    "full_basic_stats.index = full_basic_stats.index.droplevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e91e3b",
   "metadata": {},
   "source": [
    "There's also a few entries (such as Kareem Abdul-Jabbar in 1980) which have asterisks next to their names. We'll delete these asterisks so that the dataframe is easier to merge later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_basic_stats['Player'] = full_basic_stats['Player'].str.replace('*', '', regex = False)\n",
    "full_basic_stats = full_basic_stats.drop(columns = 'Rk') #dropping unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving as csv\n",
    "full_basic_stats.to_csv('data/full_basic_stats.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ddc1d",
   "metadata": {},
   "source": [
    "## Scraping Team Data\n",
    "Now, we'll repeat the same process as before to extract the team data from basketball-reference's standings page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    team_url = f'https://www.basketball-reference.com/leagues/NBA_{year}_standings.html'\n",
    "    \n",
    "    data = requests.get(team_url, headers = headers)\n",
    "    with open(f'data/team_data_{year}.html', 'w+') as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = []\n",
    "for year in years:\n",
    "    with open(f'data/team_data_{year}.html') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    soup.find('tr', class_ = 'thead').decompose()\n",
    "\n",
    "    #Scraping the eastern conference table\n",
    "    tab_html = soup.find(id = 'divs_standings_E')\n",
    "    table = pd.read_html(str(tab_html))[0]\n",
    "    table['Team'] = table['Eastern Conference']\n",
    "    table['Year'] = year\n",
    "    teams.append(table)\n",
    "\n",
    "    #scraping the western conference table\n",
    "    tab_html = soup.find(id = 'divs_standings_W')\n",
    "    table = pd.read_html(str(tab_html))[0]\n",
    "    table['Team'] = table['Western Conference']\n",
    "    table['Year'] = year\n",
    "\n",
    "    teams.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a237306",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(teams, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee182c",
   "metadata": {},
   "source": [
    "Since these data were scraped from the 'Division Standings' table, we have unwanted rows which are meant to separate the different divisions (for example, we may notice rows which say \"Atlantic Division\"). We'll simply drop these rows, as they don't contain any useful information for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fae499",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams[~teams['Team'].str.contains('Division')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c4dfb",
   "metadata": {},
   "source": [
    "We also combined the 'Western Conference' and 'Eastern Conference' columns into a single column called `Team` which contains team names from both conferences.  \n",
    "We'll now drop the two original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ab172",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams.drop(columns = ['Western Conference', 'Eastern Conference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5c62f",
   "metadata": {},
   "source": [
    "And finally, we must deal with the unwanted asterisks again, as they appear in the `Team` column here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f58e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df['Team'] = teams_df['Team'].str.replace('*', '', regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as csv\n",
    "teams.to_csv('data/team_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c990f3",
   "metadata": {},
   "source": [
    "## Scraping Advanced Player Data\n",
    "Now, we'll again repeat the process to extract each player's advanced stats from each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    players_url = f'https://www.basketball-reference.com/leagues/NBA_{year}_advanced.html'\n",
    "    \n",
    "    adv_yr = requests.get(players_url, headers = headers).text\n",
    "    \n",
    "    with open(f'Data/advanced_stats_{year}', \"w+\") as g:\n",
    "        g.write(adv_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_df = []\n",
    "for year in years:\n",
    "    with open(f'Data/advanced_stats_{year}') as g:\n",
    "        contents = g.read()\n",
    "\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    table_html = soup.find('table', class_ = 'sortable stats_table')\n",
    "\n",
    "    adv_stats_tab  = pd.read_html(str(table_html))[0]\n",
    "    adv_stats_tab['Year'] = year\n",
    "    advanced_df.append(adv_stats_tab[['Year', 'Player', 'Pos', 'Age', 'Tm', 'G', 'MP', 'PER', 'TS%', '3PAr',\n",
    "       'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%'\n",
    "                                      , 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM',\n",
    "       'DBPM', 'BPM', 'VORP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59482cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_advanced = pd.concat(advanced_df, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995458c",
   "metadata": {},
   "source": [
    "We have to handle the unwanted asterisks in player's names as we did in the basic player stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_advanced['Player'] = full_advanced['Player'].str.replace('*', '', regex = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9153e9",
   "metadata": {},
   "source": [
    "And, we'll deal with midseason trades as we did before as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac720fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_advanced = full_advanced.groupby(['Player', 'Year']).apply(player_traded)\n",
    "\n",
    "full_advanced.index = full_advanced.index.droplevel()\n",
    "full_advanced.index = full_advanced.index.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving as CSV\n",
    "full_advanced.to_csv('data/full_advanced.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6577ed0",
   "metadata": {},
   "source": [
    "## Merging dataframes\n",
    "Finally, we have all the relevant data, and we now must merge them into a single dataframe which will allow us to train our model.  \n",
    "As the first step, we'll narrow down the MVP dataframe to avoid creating redundant columns (as a few of the columns are shared with the basic stats dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_mvp = full_mvp[['Player', 'Year',\n",
    "                       'Pts Won', 'Pts Max', 'Share', 'Rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed263a2",
   "metadata": {},
   "source": [
    "Now we can merge these two dataframes.  \n",
    "  \n",
    "Note that we employ an outer merge, which will create NAN values in the columns unique to the MVP dataframe ('Pts Won', 'Pts Max', 'Share', and 'Rank') for players who don't appear in this dataframe.  \n",
    "We'll utilize the fillna method to handle this, replacing all the values with 0 (since these players by definition won no votes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = full_basic_stats.merge(narrow_mvp, left_on = ['Player', 'Year'],\n",
    "                              right_on = ['Player', 'Year'], how = 'outer').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede557a",
   "metadata": {},
   "source": [
    "In order to facilitate a merge between the `players` dataframe (which has full team names) and the `teams` dataframe (which has abbreviations), we'll use another csv file which contains all the abbreviations and team names.\n",
    "\n",
    "This data was not scraped. I simply wrote the csv file by hand as there aren't many teams present in the data and it's easy enough to assemble manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbs = pd.read_csv('data/abbreviations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e407b",
   "metadata": {},
   "source": [
    "We'll now use this dataframe to create a dictionary of abbreviation, team name pairs, and then map this to the players dataframe to create a new column containing the team's abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbs_dict = abbs.set_index('Abbreviation').to_dict(orient = 'dict')['Name']\n",
    "players['Team_abb'] = players['Tm'].map(abbs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0172b44",
   "metadata": {},
   "source": [
    "And finally, we can merge our dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = players.merge(teams_df, left_on = ['Team_abb', 'Year'], right_on = ['Team', 'Year'], how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f838cf3",
   "metadata": {},
   "source": [
    "Now, to merge with the advanced dataframe, we must again exclude certain columns already represented, as this would introduce redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_advanced = full_advanced.drop(columns = ['Pos', 'Age', 'Tm', 'G', 'MP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48a9ec",
   "metadata": {},
   "source": [
    "We can now make our final merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = merged.merge(full_advanced, \n",
    "                          left_on = ['Player', 'Year'], \n",
    "                          right_on = ['Player', 'Year'], how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc135426",
   "metadata": {},
   "source": [
    "Before we save this file, we should to convert as many columns as possible to numeric d-types, which we'll accomplish using Pandas' dataframe.apply() method and .to_numeric() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = full_table.apply(lambda x: pd.to_numeric(x, errors = 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as CSV\n",
    "full_table.to_csv('data/nba_player_stats.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832f569",
   "metadata": {},
   "source": [
    "And now, we're ready to use this data to train our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
